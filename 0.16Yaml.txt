default:
        trainer: ppo
        batch_size: 1024
        beta: 5.0e-3
        buffer_size: 10240
        epsilon: 0.2
        hidden_units: 128
        lambd: 0.95
        learning_rate: 3.0e-4
        learning_rate_schedule: linear
        max_steps: 5.0e5
        memory_size: 128
        normalize: false
        num_epoch: 3
        num_layers: 2
        time_horizon: 64
        sequence_length: 64
        summary_freq: 10000
        use_recurrent: false
        vis_encode_type: simple
        reward_signals:
            extrinsic:
                strength: 1.0
                gamma: 0.99

Axton_Battle:
        keep_checkpoints: 5
        max_steps: 10000000
        time_horizon: 32
        summary_freq: 1000
        threaded: true
        self_play:
            save_steps: 20000
            team_change: 100000
            swap_steps: 10000
            window: 200
            play_against_latest_model_ratio: 0.5
            initial_elo: 1200.0
      